[
    {
      "content": "OpenShift AI supports the following accelerators \n NVIDIA graphics processing units (GPUs) \n To use compute-heavy workloads in your models, you can enable NVIDIA graphics processing units (GPUs) in OpenShift AI \n To enable GPUs on OpenShift, you must install the NVIDIA GPU Operator. \n Habana Gaudi devices (HPUs) \n Habana, an Intel company, provides hardware accelerators intended for deep learning workloads. You can use the Habana libraries and software associated with Habana Gaudi devices available from your notebook.\n Before you can enable Habana Gaudi devices in OpenShift AI, you must install the necessary dependencies and the version of the HabanaAI Operator that matches the Habana version of the HabanaAI workbench image in your deployment. For more information about how to enable your OpenShift environment for Habana Gaudi devices, see HabanaAI Operator v1.10 for OpenShift and HabanaAI Operator v1.13 for OpenShift",
      "title": "Accelerator support in OpenShift AI"
    },
    {
      "content": "OpenShift AI provides the following model serving platforms \n Single-model serving platform \n For deploying large models such as large language models (LLMs), OpenShift AI includes a single-model serving platform that is based on the KServe component. Because each model is deployed from its own model server, the single-model serving platform helps you to deploy, monitor, scale, and maintain large models that require increased resources. \n Multi-model serving platform \n For deploying small and medium-sized models, OpenShift AI includes a multi-model serving platform that is based on the ModelMesh component. On the multi-model serving platform, you can deploy multiple models on the same model server. Each of the deployed models shares the server resources. This approach can be advantageous on OpenShift clusters that have finite compute resources or pods.",
      "title": "Model Serving in OpenShift AI"
    },
    {
      "content": "OpenShift AI integrates the following components and services \n At the service layer \n OpenShift AI dashboard \n A customer-facing dashboard that shows available and installed applications for the OpenShift AI environment as well as learning resources such as tutorials, quick starts, and documentation. Administrative users can access functionality to manage users, clusters, notebook images, accelerator profiles, and model-serving runtimes. Data scientists can use the dashboard to create projects to organize their data science work. \n Model serving \n  Data scientists can deploy trained machine-learning models to serve intelligent applications in production. After deployment, applications can send requests to the model using its deployed API endpoint \n Data science pipelines \n Data scientists can build portable machine learning (ML) workflows with Data Science Pipelines 2.0, using Docker containers. With data science pipelines, data scientists can automate workflows as they develop their data science models.  \n  Jupyter (self-managed) A self-managed application that allows data scientists to configure their own notebook server environment and develop machine learning models in JupyterLab. \n Distributed workloads \n Data scientists can use multiple nodes in parallel to train machine-learning models or process data more quickly. This approach significantly reduces the task completion time, and enables the use of larger datasets and more complex models. \n At the management layer: \n The Red Hat OpenShift AI Operator \n A meta-operator that deploys and maintains all components and sub-operators that are part of OpenShift AI.",
      "title": "Components in OpenShift AI"
    },
    {
      "content": "This article lists the Red Hat OpenShift AI (RHOAI) offerings (Self-Managed and Cloud Service). Red Hat OpenShift AI Self-Managed\n. You install OpenShift AI Self-Managed by installing the Red Hat OpenShift AI Operator and then configuring the Operator to manage standalone components of the product. RHOAI Self-Managed is supported on OpenShift Container Platform running on an x86_64 architecture. This includes support for RHOAI self-managed on managed OpenShift offerings such as OpenShift Dedicated, Red Hat OpenShift Service on AWS (ROSA), ROSA with hosted control planes, and Microsoft Azure Red Hat OpenShift. Currently, RHOAI Self-Managed is not supported on OpenShift running on other architectures (for example: ARM, IBM Power, IBM Z), and on other platforms such as OpenShift Kubernetes Engine, single node OpenShift, and MicroShift. For a full overview of the RHOAI Self-Managed life cycle and the currently supported releases, visit this page. Red Hat OpenShift AI Cloud Service \n You install OpenShift AI Cloud Service by installing the Red Hat OpenShift AI Add-on and then using the add-on to manage standalone components of the product. The add-on has a single version, reflecting the latest update of the cloud service. \n RHOAI Cloud Service is supported on OpenShift Dedicated (AWS and GCP) and on Red Hat OpenShift Service on AWS (ROSA). Currently, RHOAI Cloud Service is not supported on Microsoft Azure Red Hat OpenShift and platform services such as ROSA with hosted control planes. \n ",
      "title": "OpenShift AI Offerings"
    },
    {
      "content": "Anaconda Professional Anaconda Professional is a popular open source package distribution and management experience that is optimized for commercial use. \n IBM Watson Studio  IBM Watson Studio is a platform for embedding AI and machine learning into your business and creating custom models with your own data. \n Intel oneAPI AI Analytics Toolkit Container \tThe AI Kit is a set of AI software tools to accelerate end-to-end data science and analytics pipelines on Intel® architectures. \n Jupyter  Jupyter is a multi-user version of the notebook designed for companies, classrooms, and research labs. \n OpenVINO \tOpenVINO is an open source toolkit to help optimize deep learning performance and deploy using an inference engine onto Intel hardware. \n Pachyderm \tUse Pachyderm’s data versioning, pipeline and lineage capabilities to automate the machine learning life cycle and optimize machine learning operations. \n Starburst Enterprise Starburst Enterprise platform (SEP) is the commercial distribution of Trino, which is an open-source, Massively Parallel Processing (MPP) ANSI SQL query engine.",
      "title": "OpenShift AI Supported services"
    },
  {
    "content": "You can use the distributed workloads feature to queue, scale, and manage the resources required to run data science workloads across multiple nodes in an OpenShift cluster simultaneously. Typically, data science workloads include several types of artificial intelligence (AI) workloads, including machine learning (ML) and Python workloads. \n Distributed workloads provide the following benefits: \n  You can iterate faster and experiment more frequently because of the reduced processing time. \n You can use larger datasets, which can lead to more accurate models.\n     You can use complex models that could not be trained on a single node. \n You can submit distributed workloads at any time, and the system then schedules the distributed workload when the required resources are available. \n The distributed workloads infrastructure includes the following components: \n CodeFlare Operator \n Secures deployed Ray clusters and grants access to their URLs \n CodeFlare SDK \n Defines and controls the remote distributed compute jobs and infrastructure for any Python-based environment \n Note \n The CodeFlare SDK is not installed as part of OpenShift AI, but it is contained in some of the notebook images provided by OpenShift AI. \n KubeRay \n Manages remote Ray clusters on OpenShift for running distributed compute workloads  Kueue \n Manages quotas and how distributed workloads consume them, and manages the queueing of distributed workloads with respect to quotas",
    "title": "Distributed workloads in OpenShift AI"
  }
]