{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53f331d9-ec27-4cb2-a3c8-a9bbe768ffa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: elasticsearch 8.14.0 does not provide the extra 'vectorstore-mmr'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q einops==0.7.0 langchain==0.1.9 sentence-transformers==2.4.0 openai==1.13.3 langchain_elasticsearch langchain-community tf-keras jq tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d6c3cdf-8a46-4364-9d9f-4ca3db84658d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "#import openai\n",
    "#import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "#from getpass import getpass\n",
    "#from urllib.request import urlopen\n",
    "#from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a19abb00-febd-4599-8575-c1d6826c2d80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'elasticsearch-sample-es-default-0', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'VRnmjm8WSSOSZlE6OGzVOA', 'version': {'number': '8.14.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2afe7caceec8a26ff53817e5ed88235e90592a1b', 'build_date': '2024-07-01T22:06:58.515911606Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n",
      "[Document(page_content='OpenShift AI integrates the following components and services \\n At the service layer \\n OpenShift AI dashboard \\n A customer-facing dashboard that shows available and installed applications for the OpenShift AI environment as well as learning resources such as tutorials, quick starts, and documentation. Administrative users can access functionality to manage users, clusters, notebook images, accelerator profiles, and model-serving runtimes. Data scientists can use the dashboard to create projects to organize their data science work. \\n Model serving', metadata={'source': '/opt/app-root/src/rhoai.json', 'seq_num': 3, 'content': 'OpenShift AI integrates the following components and services \\n At the service layer \\n OpenShift AI dashboard \\n A customer-facing dashboard that shows available and installed applications for the OpenShift AI environment as well as learning resources such as tutorials, quick starts, and documentation. Administrative users can access functionality to manage users, clusters, notebook images, accelerator profiles, and model-serving runtimes. Data scientists can use the dashboard to create projects to organize their data science work. \\n Model serving \\n  Data scientists can deploy trained machine-learning models to serve intelligent applications in production. After deployment, applications can send requests to the model using its deployed API endpoint \\n Data science pipelines \\n Data scientists can build portable machine learning (ML) workflows with Data Science Pipelines 2.0, using Docker containers. With data science pipelines, data scientists can automate workflows as they develop their data science models.  \\n  Jupyter (self-managed) A self-managed application that allows data scientists to configure their own notebook server environment and develop machine learning models in JupyterLab. \\n Distributed workloads \\n Data scientists can use multiple nodes in parallel to train machine-learning models or process data more quickly. This approach significantly reduces the task completion time, and enables the use of larger datasets and more complex models. \\n At the management layer: \\n The Red\\xa0Hat OpenShift AI Operator \\n A meta-operator that deploys and maintains all components and sub-operators that are part of OpenShift AI.', 'title': 'Components in OpenShift AI'}), Document(page_content='OpenShift AI integrates the following components and services \\n At the service layer \\n OpenShift AI dashboard \\n A customer-facing dashboard that shows available and installed applications for the OpenShift AI environment as well as learning resources such as tutorials, quick starts, and documentation. Administrative users can access functionality to manage users, clusters, notebook images, accelerator profiles, and model-serving runtimes. Data scientists can use the dashboard to create projects to organize their data science work. \\n Model serving', metadata={'source': '/opt/app-root/src/rag-with-elasticsearch/rhoai.json', 'seq_num': 3, 'content': 'OpenShift AI integrates the following components and services \\n At the service layer \\n OpenShift AI dashboard \\n A customer-facing dashboard that shows available and installed applications for the OpenShift AI environment as well as learning resources such as tutorials, quick starts, and documentation. Administrative users can access functionality to manage users, clusters, notebook images, accelerator profiles, and model-serving runtimes. Data scientists can use the dashboard to create projects to organize their data science work. \\n Model serving \\n  Data scientists can deploy trained machine-learning models to serve intelligent applications in production. After deployment, applications can send requests to the model using its deployed API endpoint \\n Data science pipelines \\n Data scientists can build portable machine learning (ML) workflows with Data Science Pipelines 2.0, using Docker containers. With data science pipelines, data scientists can automate workflows as they develop their data science models.  \\n  Jupyter (self-managed) A self-managed application that allows data scientists to configure their own notebook server environment and develop machine learning models in JupyterLab. \\n Distributed workloads \\n Data scientists can use multiple nodes in parallel to train machine-learning models or process data more quickly. This approach significantly reduces the task completion time, and enables the use of larger datasets and more complex models. \\n At the management layer: \\n The Red\\xa0Hat OpenShift AI Operator \\n A meta-operator that deploys and maintains all components and sub-operators that are part of OpenShift AI.', 'title': 'Components in OpenShift AI'}), Document(page_content='This article lists the Red Hat OpenShift AI (RHOAI) offerings (Self-Managed and Cloud Service). Red Hat OpenShift AI Self-Managed', metadata={'source': '/opt/app-root/src/rag-with-elasticsearch/rhoai.json', 'seq_num': 4, 'content': 'This article lists the Red Hat OpenShift AI (RHOAI) offerings (Self-Managed and Cloud Service). Red Hat OpenShift AI Self-Managed\\n. You install OpenShift AI Self-Managed by installing the Red Hat OpenShift AI Operator and then configuring the Operator to manage standalone components of the product. RHOAI Self-Managed is supported on OpenShift Container Platform running on an x86_64 architecture. This includes support for RHOAI self-managed on managed OpenShift offerings such as OpenShift Dedicated, Red Hat OpenShift Service on AWS (ROSA), ROSA with hosted control planes, and Microsoft Azure Red Hat OpenShift. Currently, RHOAI Self-Managed is not supported on OpenShift running on other architectures (for example: ARM, IBM Power, IBM Z), and on other platforms such as OpenShift Kubernetes Engine, single node OpenShift, and MicroShift. For a full overview of the RHOAI Self-Managed life cycle and the currently supported releases, visit this page. Red Hat OpenShift AI Cloud Service \\n You install OpenShift AI Cloud Service by installing the Red Hat OpenShift AI Add-on and then using the add-on to manage standalone components of the product. The add-on has a single version, reflecting the latest update of the cloud service. \\n RHOAI Cloud Service is supported on OpenShift Dedicated (AWS and GCP) and on Red Hat OpenShift Service on AWS (ROSA). Currently, RHOAI Cloud Service is not supported on Microsoft Azure Red Hat OpenShift and platform services such as ROSA with hosted control planes. \\n ', 'title': 'OpenShift AI Offerings'}), Document(page_content='This article lists the Red Hat OpenShift AI (RHOAI) offerings (Self-Managed and Cloud Service). Red Hat OpenShift AI Self-Managed', metadata={'source': '/opt/app-root/src/rhoai.json', 'seq_num': 4, 'content': 'This article lists the Red Hat OpenShift AI (RHOAI) offerings (Self-Managed and Cloud Service). Red Hat OpenShift AI Self-Managed\\n. You install OpenShift AI Self-Managed by installing the Red Hat OpenShift AI Operator and then configuring the Operator to manage standalone components of the product. RHOAI Self-Managed is supported on OpenShift Container Platform running on an x86_64 architecture. This includes support for RHOAI self-managed on managed OpenShift offerings such as OpenShift Dedicated, Red Hat OpenShift Service on AWS (ROSA), ROSA with hosted control planes, and Microsoft Azure Red Hat OpenShift. Currently, RHOAI Self-Managed is not supported on OpenShift running on other architectures (for example: ARM, IBM Power, IBM Z), and on other platforms such as OpenShift Kubernetes Engine, single node OpenShift, and MicroShift. For a full overview of the RHOAI Self-Managed life cycle and the currently supported releases, visit this page. Red Hat OpenShift AI Cloud Service \\n You install OpenShift AI Cloud Service by installing the Red Hat OpenShift AI Add-on and then using the add-on to manage standalone components of the product. The add-on has a single version, reflecting the latest update of the cloud service. \\n RHOAI Cloud Service is supported on OpenShift Dedicated (AWS and GCP) and on Red Hat OpenShift Service on AWS (ROSA). Currently, RHOAI Cloud Service is not supported on Microsoft Azure Red Hat OpenShift and platform services such as ROSA with hosted control planes. \\n ', 'title': 'OpenShift AI Offerings'})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "client = Elasticsearch(['https://elasticsearch-sample-myrag.apps.cluster-2mw5c.sandbox1301.opentlc.com'], basic_auth=('elastic', '7i2W81WNwKU44E4wL8E2a0gz'), verify_certs=False)\n",
    "\n",
    "print(client.info())\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "vector_store = ElasticsearchStore(es_connection= client,\n",
    "    index_name=\"rhoai_index\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"content\"] = record.get(\"content\")\n",
    "    metadata[\"title\"] = record.get(\"title\")\n",
    "    return metadata\n",
    "\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=\"rhoai.json\",\n",
    "    jq_schema=\".[]\",\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=128, chunk_overlap=64\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "\n",
    "documents = vector_store.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    index_name=\"rhoai_index\",\n",
    "    es_connection= client\n",
    ")\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search(\"can you list OpenShift AI offerings\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86e96410-60ba-4f57-8a0e-ff90b2b40378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'elasticsearch-sample-es-default-0', 'cluster_name': 'elasticsearch-sample', 'cluster_uuid': 'VRnmjm8WSSOSZlE6OGzVOA', 'version': {'number': '8.14.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '2afe7caceec8a26ff53817e5ed88235e90592a1b', 'build_date': '2024-07-01T22:06:58.515911606Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######################################################################\n",
    "#      INITIALIZE ELASTICSEARCH CONNECTION AND VECTOR STORE           #\n",
    "#######################################################################\n",
    "\n",
    "client = Elasticsearch(['https://elasticsearch-sample-myrag.apps.cluster-2mw5c.sandbox1301.opentlc.com'], basic_auth=('elastic', '7i2W81WNwKU44E4wL8E2a0gz'), verify_certs=False)\n",
    "\n",
    "print(client.info())\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "store = ElasticsearchStore(es_connection= client,\n",
    "    index_name=\"rhoai_index\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "#######################################################################\n",
    "#      INITIALIZE LLM DEPLOYMENT PARAMETERS                           #\n",
    "#######################################################################\n",
    "\n",
    "INFERENCE_SERVER_URL = f\"http://llm-predictor.myrag.svc.cluster.local:8080/v1\"\n",
    "MODEL_NAME = \"llm\"\n",
    "MAX_TOKENS=1024\n",
    "TOP_P=0.95\n",
    "TEMPERATURE=0.01\n",
    "PRESENCE_PENALTY=1.03\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "505d3d75-c781-4983-b9b3-64ef986a4d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### [INST] \n",
    "Instruction: Answer the question based on your \n",
    "OpenShift AI knowledge. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    " \"\"\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=INFERENCE_SERVER_URL,\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "            ),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True\n",
    "        )\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "901fae82-7515-4b16-8222-bbcb47f29665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenShift AI supports two types of accelerators for running machine learning workloads: NVIDIA graphics processing units (GPUs) and Habana Gaudi hardware processing units (HPUs). To use GPUs, you need to install the NVIDIA GPU Operator. For HPUs, you can use the Habana libraries and software associated with Habana Gaudi devices available from your notebook."
     ]
    }
   ],
   "source": [
    "question = \"What accelerators are supported on OpenShift AI?\"\n",
    "result = qa_chain.invoke({\"query\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dddd1a-e701-434a-bc34-d52e8e7db40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
